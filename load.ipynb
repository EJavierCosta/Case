{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1f9965-51c1-4963-b979-a0c528ff157e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, desc, lit, when, datediff, sum, round, avg, regexp_replace, to_date, to_timestamp, hour, struct, collect_list, to_json\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc6c1df-b532-43ba-9f72-d012b7b927c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def spark_df_to_json_serializable(df, limit=100):\n",
    "    \"\"\"\n",
    "    Converte um DataFrame Spark em uma string JSON que pode ser enviada via API.\n",
    "    Retorna uma lista de dicionários Python.\n",
    "    \"\"\"\n",
    "    # Limita o DataFrame para não enviar dados excessivos\n",
    "    limited_df = df.limit(limit)\n",
    "    \n",
    "    # Usa o método nativo do Spark para converter para uma string JSON de um array de objetos\n",
    "    json_string = limited_df.agg(\n",
    "        to_json(collect_list(struct(*limited_df.columns)))\n",
    "    ).first()[0]\n",
    "    \n",
    "    # Se o resultado for nulo ou vazio, retorna uma lista vazia\n",
    "    if not json_string:\n",
    "        return []\n",
    "        \n",
    "    # Converte a string JSON de volta para um objeto Python (lista de dicionários)\n",
    "    return json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215703fa-dbb2-4edf-b96e-c26d98590c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pedidos_ = spark.read.csv(\n",
    "    \"/Volumes/transacional/case_gocase/tmp/Case Dados - Pedidos.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_itens = spark.read.csv(\n",
    "    \"/Volumes/transacional/case_gocase/tmp/Business Case Dados Itens.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_supply = spark.read.csv(\n",
    "    \"/Volumes/transacional/case_gocase/tmp/Business Case Dados Supply.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "# display(df_pedidos.limit(5))\n",
    "# display(df_itens.limit(5))\n",
    "# display(df_supply.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339e9b2b-56f7-4357-80df-cfce36294aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Renomear colunas (sem alterações) ---\n",
    "df_pedidos = (df_pedidos_\n",
    "    .withColumnRenamed(\"CÃ³digo de Rastreio\", \"codigo_rastreio\")\n",
    "    .withColumnRenamed(\"Valor de NF (R$)\", \"valor_nf\")\n",
    "    .withColumnRenamed(\"Frete Cobrado do Cliente (R$)\", \"frete_cliente\")\n",
    "    .withColumnRenamed(\"Frete cobrado pela transportadora (R$)\", \"frete_transportadora\")\n",
    "    .withColumnRenamed(\"NÃºmero da NF\", \"numero_nf\")\n",
    "    .withColumnRenamed(\"Status do Pedido\", \"status_pedido\")\n",
    "    .withColumnRenamed(\"Prazo para Sair do CD\", \"prazo_saida_cd\")\n",
    "    .withColumnRenamed(\"Enviado em:\", \"data_envio\")\n",
    "    .withColumnRenamed(\"Entregue para o cliente em:\", \"data_entrega\")\n",
    "    .withColumnRenamed(\"Prazo a transportadora entregar no cliente\", \"prazo_entrega_transportadora\")\n",
    "    .withColumnRenamed(\"NÃºmero de Itens no Pedido\", \"num_itens\")\n",
    "    .withColumnRenamed(\"Peso (kg)\", \"peso_kg\")\n",
    "    .withColumnRenamed(\"id\", \"order_id\")\n",
    ")\n",
    "\n",
    "# --- 2. Converter colunas numéricas (sem alterações) ---\n",
    "cols_to_convert_pedidos = [\"valor_nf\", \"frete_cliente\", \"frete_transportadora\", \"peso_kg\"]\n",
    "for column in cols_to_convert_pedidos:\n",
    "    df_pedidos = df_pedidos.withColumn(column, regexp_replace(regexp_replace(col(column), \"\\\\.\", \"\"), \",\", \".\").cast(\"double\"))\n",
    "\n",
    "cols_to_convert_itens = [\"material_weight_kg\", \"price\"]\n",
    "for column in cols_to_convert_itens:\n",
    "    df_itens = df_itens.withColumn(column, regexp_replace(regexp_replace(col(column), \"\\\\.\", \"\"), \",\", \".\").cast(\"double\"))\n",
    "\n",
    "df_supply = df_supply.withColumn(\"quantity\", regexp_replace(regexp_replace(col(\"quantity\"), \"\\\\.\", \"\"), \",\", \".\").cast(\"double\"))\n",
    "\n",
    "# --- 3. NOVA ABORDAGEM PARA DATAS ---\n",
    "\n",
    "# Dicionário para \"traduzir\" os meses, será usado para ambos os formatos\n",
    "month_map = {\n",
    "    \"jan.,\": \"Jan\", \"fev.,\": \"Feb\", \"mar.,\": \"Mar\", \"abr.,\": \"Apr\",\n",
    "    \"mai.,\": \"May\", \"jun.,\": \"Jun\", \"jul.,\": \"Jul\", \"ago.,\": \"Aug\",\n",
    "    \"set.,\": \"Sep\", \"out.,\": \"Oct\", \"nov.,\": \"Nov\", \"dez.,\": \"Dec\"\n",
    "}\n",
    "\n",
    "# 3.1: Tratar colunas que SÓ TÊM DATA (formato \"10 mar., 2025\")\n",
    "date_only_cols = [\"prazo_saida_cd\", \"data_envio\", \"data_entrega\", \"prazo_entrega_transportadora\"]\n",
    "format_date_only = \"d MMM yyyy\"\n",
    "\n",
    "for date_col in date_only_cols:\n",
    "    # Cria uma expressão inicial para a coluna\n",
    "    expr = col(date_col)\n",
    "    # Aplica a substituição de todos os meses\n",
    "    for pt_month, en_month in month_map.items():\n",
    "        expr = regexp_replace(expr, pt_month, en_month)\n",
    "    \n",
    "    # Atualiza a coluna no DataFrame, convertendo a string tratada para o tipo Date\n",
    "    df_pedidos = df_pedidos.withColumn(date_col, to_date(expr, format_date_only))\n",
    "\n",
    "\n",
    "# 3.2: Tratar a coluna `created_at` (formato \"28 fev., 2025, 23:42\")\n",
    "format_datetime = \"d MMM yyyy, HH:mm\"\n",
    "\n",
    "# Cria a expressão inicial para a coluna created_at\n",
    "expr_ts = col(\"created_at\")\n",
    "# Aplica a substituição de todos os meses\n",
    "for pt_month, en_month in month_map.items():\n",
    "    expr_ts = regexp_replace(expr_ts, pt_month, en_month)\n",
    "\n",
    "# Aplica as transformações em cadeia para criar as novas colunas\n",
    "df_pedidos = (df_pedidos\n",
    "    # 1. Cria uma coluna temporária com o timestamp completo\n",
    "    .withColumn(\"created_at_ts\", to_timestamp(expr_ts, format_datetime))\n",
    "    # 2. Cria a coluna de DATA a partir do timestamp\n",
    "    .withColumn(\"created_at_data\", to_date(col(\"created_at_ts\")))\n",
    "    # 3. Cria a coluna de HORA a partir do timestamp\n",
    "    .withColumn(\"created_at_hour\", hour(col(\"created_at_ts\")))\n",
    "    # 4. Remove a coluna original e a temporária\n",
    "    .drop(\"created_at\", \"created_at_ts\")\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Criar DataFrame base para análise ---\n",
    "df_vendas = df_pedidos.join(df_itens, \"order_id\", \"inner\")\n",
    "\n",
    "\n",
    "# --- 5. Verificação Final ---\n",
    "\n",
    "\n",
    "# df_vendas.printSchema()\n",
    "\n",
    "\n",
    "# display(df_pedidos.limit(5))\n",
    "\n",
    "\n",
    "# display(df_vendas.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd03e14-001d-4b2c-b71f-8d55d3861ea9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "send"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Execute e colete os resultados de cada análise ---\n",
    "print(\"Coletando resultados...\")\n",
    "\n",
    "# Análise 1.1: Oscilações nas Vendas e Impacto de Promoções\n",
    "vendas_diarias_resultado = spark_df_to_json_serializable(vendas_diarias, limit=31)\n",
    "print(\"-> Resultados de vendas diárias coletados.\")\n",
    "\n",
    "# Análise 1.2: Impacto da Promoção de Frete Grátis\n",
    "analise_frete_resultado = spark_df_to_json_serializable(analise_frete, limit=10)\n",
    "print(\"-> Resultados de desempenho por transportadora coletados.\")\n",
    "\n",
    "# Análise 1.3: Quais categorias ou produtos possuem maior impacto no faturamento?\n",
    "faturamento_por_categoria_resultado = spark_df_to_json_serializable(faturamento_por_categoria, limit=50)\n",
    "faturamento_por_produto_resultado = spark_df_to_json_serializable(faturamento_por_produto, limit=50)\n",
    "print(\"-> Resultados da análise de cancelamento coletados.\")\n",
    "\n",
    "# Análise 2.1: Identificar Produtos Críticos em Ruptura\n",
    "total_itens_unicos_resultado = spark_df_to_json_serializable(total_itens_unicos, limit=10)\n",
    "itens_unicos_em_ruptura_resultado = spark_df_to_json_serializable(itens_unicos_em_ruptura, limit=10)\n",
    "percentual_ruptura_resultado = spark_df_to_json_serializable(percentual_ruptura, limit=10)\n",
    "\n",
    "# Análise 2.2: Priorizar a Reposição: Cruzando Ruptura com Demanda\n",
    "ruptura_critica_resultado = spark_df_to_json_serializable(ruptura_critica_top20, limit=20)\n",
    "ruptura_critica_faturamento_resultado = spark_df_to_json_serializable(ruptura_critica_faturamento_top20, limit=20)\n",
    "\n",
    "# Análise 3.1: Calcular Tempos do Ciclo de Entrega\n",
    "medias_logisticas_resultado = spark_df_to_json_serializable(medias_logisticas, limit=10)\n",
    "\n",
    "# Analise 3.2: Análise de Atrasos e Desempenho por Transportadora\n",
    "desempenho_transportadoras_resultado = spark_df_to_json_serializable(desempenho_transportadoras, limit=10)\n",
    "\n",
    "# --- 2. Crie o \"pacote\" de dados (dicionário mestre) ---\n",
    "# Cada chave do dicionário representa um relatório diferente.\n",
    "master_payload = {\n",
    "    \"relatorio_vendas_diarias\": vendas_diarias_resultado,\n",
    "    \"relatorio_analise_fretes\": analise_frete_resultado,\n",
    "    \"faturamento_por_categoria_resultado\": faturamento_por_categoria_resultado\n",
    "    \"faturamento_por_produto_resultado\": faturamento_por_produto_resultado\n",
    "    \"total_itens_unicos_resultado\": total_itens_unicos_resultado,\n",
    "    \"itens_unicos_em_ruptura_resultado\": itens_unicos_em_ruptura_resultado,\n",
    "    \"percentual_ruptura_resultado\": percentual_ruptura_resultado,\n",
    "    \"ruptura_critica_resultado\": ruptura_critica_resultado,\n",
    "    \"ruptura_critica_faturamento_resultado\": ruptura_critica_faturamento_resultado,\n",
    "    \"medias_logisticas_resultado\": medias_logisticas_resultado,\n",
    "    \"desempenho_transportadoras_resultado\": desempenho_transportadoras_resultado\n",
    "}\n",
    "\n",
    "# --- 3. Envie o pacote completo para o n8n ---\n",
    "n8n_webhook_url = \"https://emanoeljavier.app.n8n.cloud/webhook-test/2c0caced-9c0d-40b0-838f-04daab4f62ca\" # <-- USE A URL DE PRODUÇÃO\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "try:\n",
    "    print(\"\\nEnviando pacote de dados completo para o n8n...\")\n",
    "    response = requests.post(n8n_webhook_url, data=json.dumps(master_payload), headers=headers)\n",
    "    response.raise_for_status()\n",
    "    print(\"✅ Pacote de dados enviado com sucesso para o n8n!\")\n",
    "    print(f\"Resposta do n8n: {response.json()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Falha ao enviar dados para o n8n: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
