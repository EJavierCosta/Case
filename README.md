# ğŸ“Š Business Case - Dados (E-commerce B2C)

## ğŸŒ Contexto da Empresa
Somos um **e-commerce B2C em rÃ¡pido crescimento**, com mÃºltiplas categorias de produtos e uma operaÃ§Ã£o logÃ­stica robusta.  
Atualmente, enfrentamos desafios relacionados a **vendas, supply chain e logÃ­stica**, e buscamos estruturar nossa Ã¡rea de dados para apoiar decisÃµes estratÃ©gicas.

---

## ğŸ¯ Objetivo do Case
O desafio consiste em realizar um **diagnÃ³stico inicial das bases de dados** fornecidas (Pedidos, Itens dos Pedidos e Supply), gerando **insights estratÃ©gicos** que auxiliem a empresa a:

- Melhorar a **conversÃ£o de vendas**  
- Reduzir **rupturas de estoque**  
- Otimizar a **logÃ­stica e prazos de entrega**  


---

## ğŸ› ï¸ Tecnologias Utilizadas
- **Python** â†’ Scripts de ETL e anÃ¡lises
- **PySpark** â†’ Processamento de grandes volumes de dados
- **Google Sheets API** â†’ extraÃ§Ã£o de dados
- **Databricks** â†’ Unity Catalog, Jobs, Notebooks, Delta Lake, Dashboards 
- **n8n** â†’ automaÃ§Ã£o (query â†’ PDF â†’ eâ€‘mail)
- **GitHub** â†’ Versionamento e colaboraÃ§Ã£o

---

## ğŸ§­ VisÃ£o Geral

- **Fonte**: duas bases em Google Sheets (Itens/Supply e Pedidos)
- **IngestÃ£o**: via **API** para o **Databricks Catalog**
- **Processamento**: **Apache Spark** em job agendado
- **PersistÃªncia**: **Delta Lake** (tabelas versionadas)
- **Consumo**:
  - **Dashboards Databricks**
  - **n8n** â†’ gera **PDF** e envia **eâ€‘mail** (RelatÃ³rio IA)

## ğŸ—ºï¸ Arquitetura

```mermaid

graph LR
    %% DireÃ§Ã£o horizontal (left â†’ right)
    classDef note fill:#fff3cd,stroke:#d39e00,color:#333,font-size:10px;

    subgraph GOOGLE_SHEETS["GOOGLE SHEETS"]
        A["DB CSV <br/> ITENS/SUPPLY"]
        B["DB CSV <br/> PEDIDOS"]
    end

    subgraph DATABRICKS["DATABRICKS"]
        D["DATABRICKS CATALOG"]
        E{"SPARK"}
        F[("DELTA LAKE")]
        G["DASHBOARDS DATABRICKS"]

        %% Texto de agendamento isolado
        J["AGENDAMENTO:<br/>10:00 do dia 01 de cada mÃªs"]:::note
    end

    subgraph n8n["n8n"]
        H["WORKFLOW RELATÃ“RIO IA"]
        I[/PDF EMAIL/]
    end

    A -- API --> D
    B -- API --> D
    D --> E
    E --> F
    F --> G
    F --> H
    H --> I

```

---

## ğŸ“‚ Estrutura de Pastas 
```
Case/
â”œâ”€â”€ dashboard_databricks/ 
â”‚   â”œâ”€â”€ Dash_Gocase.lvdash.json       # dashboard databricks
â”‚   â””â”€â”€ Dashboard_Databricks.jpg      # print dashboard databricks
â”‚
â”œâ”€â”€ job_databricks/             
â”‚   â”œâ”€â”€ Case_Gocase.json        # json com o job do databricks para reproduÃ§Ã£o.
â”‚   â””â”€â”€ job_databricks.jpg      # print job databricks
â”‚    
â”œâ”€â”€ workflow/                   
â”‚   â”œâ”€â”€ workflow_n8n.json    # json workflow do n8n para reproduÃ§Ã£o.
â”‚   â””â”€â”€ workflow_n8n.jpg     # print workflow n8n
â”‚  
â”œâ”€â”€ analise.ipynb            # notebook com os codigos em pyspark para anÃ¡lise
â”œâ”€â”€ ingestor.ipynb           # notebook com os codigos de extraÃ§Ã£o csv
â”œâ”€â”€ utils.py                 # funÃ§Ãµes python
â”œâ”€â”€ Case_Gocase.drawio       # arquitetura
â””â”€â”€ README.md

```

## ğŸš€ Como rodar

### 1) IngestÃ£o (Google Sheets â†’ CSV-Catalog)
- Execute o notebook `ingestor.ipynb` em um **cluster** databricks.

### 2) AnÃ¡lises (CSV-Catalog â†’ Delta Lake)
- Execute `analise.ipynb `.

### 3) Agendamento (Monthly)
- Crie um **Job** agendado no Databricks com `job_databricks/Case_Gocase.json`. 

### 4) Dashboards
- Use o `Dash_Gocase.lvdash.json` no **Databricks** para gerar o dashboard.

### 5) n8n â€” RelatÃ³rio IA (PDF â†’ Eâ€‘mail)
- Importe `workflow/workflow_n8n.json` no n8n para criaÃ§Ã£o do work flow.
- Configure as credenciais das APIs
---

## ğŸ“¬ Contato
**Autor:** Emanoel Javier de Araujo Costa  
**GitHub:** https://github.com/EJavierCosta
