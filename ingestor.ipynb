{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fe5bd2-8dd3-4c42-87d0-2fddd8ba75a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39da8da-e4e1-462a-b902-fe2b3d219b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CSV_URL = (\n",
    "    \"https://docs.google.com/spreadsheets/d/1reziwqueMdfNA-3Vgz-JxVK_XXaB99nuOvHSdC0wBnw/export?format=csv&gid=919246905\"\n",
    "   # \"https://docs.google.com/spreadsheets/d/1reziwqueMdfNA-3Vgz-JxVK_XXaB99nuOvHSdC0wBnw/edit?usp=sharing\"\n",
    "   #\"https://docs.google.com/spreadsheets/d/1reziwqueMdfNA-3Vgz-JxVK_XXaB99nuOvHSdC0wBnw/edit?gid=919246905#gid=919246905\"\n",
    ")\n",
    "dbfs_path = \"/Volumes/transacional/case_gocase/tmp/Case Dados - Pedidos.csv\"\n",
    "\n",
    "response = requests.get(CSV_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(dbfs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55986302-c899-4f11-b202-a7caa8097fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CSV_URL = (\n",
    "    \"https://docs.google.com/spreadsheets/d/1c7uf9pHUN5JvNJHuueXLkaGkX8vvslWd5-NiPYoZ8V0/export?format=csv&gid=224192872\"\n",
    "    #\"https://docs.google.com/spreadsheets/d/1c7uf9pHUN5JvNJHuueXLkaGkX8vvslWd5-NiPYoZ8V0/edit?usp=sharing\"\n",
    "    #https://docs.google.com/spreadsheets/d/1c7uf9pHUN5JvNJHuueXLkaGkX8vvslWd5-NiPYoZ8V0/edit?gid=1760558779#gid=1760558779\n",
    ")\n",
    "dbfs_path = \"/Volumes/transacional/case_gocase/tmp/Business Case Dados Supply.csv\"\n",
    "\n",
    "response = requests.get(CSV_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(dbfs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d62bf2a-7532-4302-b05f-3929316b96c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CSV_URL = (\n",
    "    \"https://docs.google.com/spreadsheets/d/1c7uf9pHUN5JvNJHuueXLkaGkX8vvslWd5-NiPYoZ8V0/export?format=csv&gid=1760558779\"\n",
    "    #\"https://docs.google.com/spreadsheets/d/1c7uf9pHUN5JvNJHuueXLkaGkX8vvslWd5-NiPYoZ8V0/edit?usp=sharing\"\n",
    "    #https://docs.google.com/spreadsheets/d/1c7uf9pHUN5JvNJHuueXLkaGkX8vvslWd5-NiPYoZ8V0/edit?gid=1760558779#gid=1760558779\n",
    ")\n",
    "dbfs_path = \"/Volumes/transacional/case_gocase/tmp/Business Case Dados Itens.csv\"\n",
    "\n",
    "response = requests.get(CSV_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(dbfs_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b6c596-e559-4428-ac11-80846b8402f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pedidos = spark.read.csv(\n",
    "    \"/Volumes/transacional/case_gocase/tmp/Case Dados - Pedidos.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_itens = spark.read.csv(\n",
    "    \"/Volumes/transacional/case_gocase/tmp/Business Case Dados Itens.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_supply = spark.read.csv(\n",
    "    \"/Volumes/transacional/case_gocase/tmp/Business Case Dados Supply.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "display(df_pedidos.limit(5))\n",
    "display(df_itens.limit(5))\n",
    "display(df_supply.limit(5))\n",
    "print(dict(df_pedidos.dtypes)[\"created_at\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "469522c0-02a8-46ed-9fd6-d86105573eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, regexp_replace, datediff, coalesce, to_timestamp, lit\n",
    "\n",
    "# --- 1. Renomear colunas problemáticas para facilitar o uso ---\n",
    "df_pedidos = (df_pedidos\n",
    "    .withColumnRenamed(\"CÃ³digo de Rastreio\", \"codigo_rastreio\")\n",
    "    .withColumnRenamed(\"Valor de NF (R$)\", \"valor_nf\")\n",
    "    .withColumnRenamed(\"Frete Cobrado do Cliente (R$)\", \"frete_cliente\")\n",
    "    .withColumnRenamed(\"Frete cobrado pela transportadora (R$)\", \"frete_transportadora\")\n",
    "    .withColumnRenamed(\"NÃºmero da NF\", \"numero_nf\")\n",
    "    .withColumnRenamed(\"Status do Pedido\", \"status_pedido\")\n",
    "    .withColumnRenamed(\"Prazo para Sair do CD\", \"prazo_saida_cd\")\n",
    "    .withColumnRenamed(\"Enviado em:\", \"data_envio\")\n",
    "    .withColumnRenamed(\"Entregue para o cliente em:\", \"data_entrega\")\n",
    "    .withColumnRenamed(\"Prazo a transportadora entregar no cliente\", \"prazo_entrega_transportadora\")\n",
    "    .withColumnRenamed(\"NÃºmero de Itens no Pedido\", \"num_itens\")\n",
    "    .withColumnRenamed(\"Peso (kg)\", \"peso_kg\")\n",
    "    .withColumnRenamed(\"id\", \"order_id\") # Renomear para join com df_itens\n",
    ")\n",
    "\n",
    "# --- 2. Converter colunas de valores monetários e numéricos (String para Double) ---\n",
    "cols_to_convert_pedidos = [\"valor_nf\", \"frete_cliente\", \"frete_transportadora\", \"peso_kg\"]\n",
    "for column in cols_to_convert_pedidos:\n",
    "    df_pedidos = df_pedidos.withColumn(\n",
    "        column,\n",
    "        regexp_replace(\n",
    "            regexp_replace(col(column), \"\\\\.\", \"\"),\n",
    "            \",\",\n",
    "            \".\"\n",
    "        ).cast(\"double\")\n",
    "    )\n",
    "\n",
    "cols_to_convert_itens = [\"material_weight_kg\", \"price\"]\n",
    "for column in cols_to_convert_itens:\n",
    "    df_itens = df_itens.withColumn(\n",
    "        column,\n",
    "        regexp_replace(\n",
    "            regexp_replace(col(column), \"\\\\.\", \"\"),\n",
    "            \",\",\n",
    "            \".\"\n",
    "        ).cast(\"double\")\n",
    "    )\n",
    "\n",
    "df_supply = df_supply.withColumn(\n",
    "    \"quantity\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(col(\"quantity\"), \"\\\\.\", \"\"),\n",
    "        \",\",\n",
    "        \".\"\n",
    "    ).cast(\"double\")\n",
    ")\n",
    "\n",
    "# --- 3. CORREÇÃO FINAL E MAIS ROBUSTA: Converter colunas de data com múltiplos formatos ---\n",
    "\n",
    "date_cols = [\"created_at\", \"prazo_saida_cd\", \"data_envio\", \"data_entrega\", \"prazo_entrega_transportadora\"]\n",
    "\n",
    "# Dicionário para \"traduzir\" os meses em português\n",
    "month_map = {\n",
    "    \"jan.,\": \"Jan\", \"fev.,\": \"Feb\", \"mar.,\": \"Mar\", \"abr.,\": \"Apr\",\n",
    "    \"mai.,\": \"May\", \"jun.,\": \"Jun\", \"jul.,\": \"Jul\", \"ago.,\": \"Aug\",\n",
    "    \"set.,\": \"Sep\", \"out.,\": \"Oct\", \"nov.,\": \"Nov\", \"dez.,\": \"Dec\"\n",
    "}\n",
    "\n",
    "# Definindo TODOS os formatos de data encontrados no arquivo\n",
    "format_com_hora_br = \"d MMM yyyy, HH:mm\"\n",
    "format_iso = \"yyyy-MM-dd HH:mm:ss\"\n",
    "format_sem_hora_br = \"d MMM yyyy\" # Novo formato que você identificou\n",
    "\n",
    "# Loop para corrigir e converter cada coluna de data\n",
    "for date_col in date_cols:\n",
    "    # 1. Cria uma coluna temporária com os meses traduzidos para os formatos em português\n",
    "    temp_col_br = col(date_col)\n",
    "    \n",
    "    # CORREÇÃO DO ERRO ATUAL: Garantir que o loop use a variável correta (pt_month)\n",
    "    for pt_month, en_month in month_map.items():\n",
    "        temp_col_br = regexp_replace(temp_col_br, pt_month, en_month)\n",
    "    \n",
    "    # 2. Usa coalesce para tentar os três formatos em ordem, retornando o primeiro que funcionar\n",
    "    df_pedidos = df_pedidos.withColumn(\n",
    "        date_col,\n",
    "        coalesce(\n",
    "            to_timestamp(temp_col_br, lit(format_com_hora_br)),\n",
    "            to_timestamp(temp_col_br, lit(format_iso)),\n",
    "            to_timestamp(temp_col_br, lit(format_sem_hora_br))\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- 4. AJUSTE: Criar DataFrame base para análise APÓS a limpeza ---\n",
    "df_vendas = df_pedidos.join(df_itens, \"order_id\", \"inner\")\n",
    "\n",
    "# --- 5. Verificação Final ---\n",
    "\n",
    "df_vendas.printSchema()\n",
    "df_supply.printSchema()\n",
    "display(df_vendas.limit(5))\n",
    "display(df_pedidos.limit(5))\n",
    "display(df_supply.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7bc9ccf-75d5-4c95-af12-9ac361720d7a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760192069479}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, to_timestamp, hour\n",
    "\n",
    "# --- 1. Renomear colunas (sem alterações) ---\n",
    "df_pedidos = (df_pedidos\n",
    "    .withColumnRenamed(\"CÃ³digo de Rastreio\", \"codigo_rastreio\")\n",
    "    .withColumnRenamed(\"Valor de NF (R$)\", \"valor_nf\")\n",
    "    .withColumnRenamed(\"Frete Cobrado do Cliente (R$)\", \"frete_cliente\")\n",
    "    .withColumnRenamed(\"Frete cobrado pela transportadora (R$)\", \"frete_transportadora\")\n",
    "    .withColumnRenamed(\"NÃºmero da NF\", \"numero_nf\")\n",
    "    .withColumnRenamed(\"Status do Pedido\", \"status_pedido\")\n",
    "    .withColumnRenamed(\"Prazo para Sair do CD\", \"prazo_saida_cd\")\n",
    "    .withColumnRenamed(\"Enviado em:\", \"data_envio\")\n",
    "    .withColumnRenamed(\"Entregue para o cliente em:\", \"data_entrega\")\n",
    "    .withColumnRenamed(\"Prazo a transportadora entregar no cliente\", \"prazo_entrega_transportadora\")\n",
    "    .withColumnRenamed(\"NÃºmero de Itens no Pedido\", \"num_itens\")\n",
    "    .withColumnRenamed(\"Peso (kg)\", \"peso_kg\")\n",
    "    .withColumnRenamed(\"id\", \"order_id\")\n",
    ")\n",
    "\n",
    "# --- 2. Converter colunas numéricas (sem alterações) ---\n",
    "cols_to_convert_pedidos = [\"valor_nf\", \"frete_cliente\", \"frete_transportadora\", \"peso_kg\"]\n",
    "for column in cols_to_convert_pedidos:\n",
    "    df_pedidos = df_pedidos.withColumn(column, regexp_replace(regexp_replace(col(column), \"\\\\.\", \"\"), \",\", \".\").cast(\"double\"))\n",
    "\n",
    "cols_to_convert_itens = [\"material_weight_kg\", \"price\"]\n",
    "for column in cols_to_convert_itens:\n",
    "    df_itens = df_itens.withColumn(column, regexp_replace(regexp_replace(col(column), \"\\\\.\", \"\"), \",\", \".\").cast(\"double\"))\n",
    "\n",
    "df_supply = df_supply.withColumn(\"quantity\", regexp_replace(regexp_replace(col(\"quantity\"), \"\\\\.\", \"\"), \",\", \".\").cast(\"double\"))\n",
    "\n",
    "# --- 3. NOVA ABORDAGEM PARA DATAS ---\n",
    "\n",
    "# Dicionário para \"traduzir\" os meses, será usado para ambos os formatos\n",
    "month_map = {\n",
    "    \"jan.,\": \"Jan\", \"fev.,\": \"Feb\", \"mar.,\": \"Mar\", \"abr.,\": \"Apr\",\n",
    "    \"mai.,\": \"May\", \"jun.,\": \"Jun\", \"jul.,\": \"Jul\", \"ago.,\": \"Aug\",\n",
    "    \"set.,\": \"Sep\", \"out.,\": \"Oct\", \"nov.,\": \"Nov\", \"dez.,\": \"Dec\"\n",
    "}\n",
    "\n",
    "# 3.1: Tratar colunas que SÓ TÊM DATA (formato \"10 mar., 2025\")\n",
    "date_only_cols = [\"prazo_saida_cd\", \"data_envio\", \"data_entrega\", \"prazo_entrega_transportadora\"]\n",
    "format_date_only = \"d MMM yyyy\"\n",
    "\n",
    "for date_col in date_only_cols:\n",
    "    # Cria uma expressão inicial para a coluna\n",
    "    expr = col(date_col)\n",
    "    # Aplica a substituição de todos os meses\n",
    "    for pt_month, en_month in month_map.items():\n",
    "        expr = regexp_replace(expr, pt_month, en_month)\n",
    "    \n",
    "    # Atualiza a coluna no DataFrame, convertendo a string tratada para o tipo Date\n",
    "    df_pedidos = df_pedidos.withColumn(date_col, to_date(expr, format_date_only))\n",
    "\n",
    "\n",
    "# 3.2: Tratar a coluna `created_at` (formato \"28 fev., 2025, 23:42\")\n",
    "format_datetime = \"d MMM yyyy, HH:mm\"\n",
    "\n",
    "# Cria a expressão inicial para a coluna created_at\n",
    "expr_ts = col(\"created_at\")\n",
    "# Aplica a substituição de todos os meses\n",
    "for pt_month, en_month in month_map.items():\n",
    "    expr_ts = regexp_replace(expr_ts, pt_month, en_month)\n",
    "\n",
    "# Aplica as transformações em cadeia para criar as novas colunas\n",
    "df_pedidos = (df_pedidos\n",
    "    # 1. Cria uma coluna temporária com o timestamp completo\n",
    "    .withColumn(\"created_at_ts\", to_timestamp(expr_ts, format_datetime))\n",
    "    # 2. Cria a coluna de DATA a partir do timestamp\n",
    "    .withColumn(\"created_at_data\", to_date(col(\"created_at_ts\")))\n",
    "    # 3. Cria a coluna de HORA a partir do timestamp\n",
    "    .withColumn(\"created_at_hour\", hour(col(\"created_at_ts\")))\n",
    "    # 4. Remove a coluna original e a temporária\n",
    "    .drop(\"created_at\", \"created_at_ts\")\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. Criar DataFrame base para análise ---\n",
    "df_vendas = df_pedidos.join(df_itens, \"order_id\", \"inner\")\n",
    "\n",
    "\n",
    "# --- 5. Verificação Final ---\n",
    "\n",
    "\n",
    "df_vendas.printSchema()\n",
    "\n",
    "\n",
    "display(df_pedidos.limit(100))\n",
    "\n",
    "\n",
    "display(df_vendas.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2061708-c0fb-4c39-88ad-fb897a989652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "contagem_nulos = df_pedidos.filter(col(\"prazo_entrega_transportadora\").isNull()).count()\n",
    "\n",
    "print(contagem_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82532af0-43bf-43fe-ab7b-db7197671a74",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760192092137}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_vendas.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28afb425-df82-4faa-818c-6bfd7c08f395",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Análise 1: Vendas ao Longo do Tempo"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, count, date_trunc, avg, when\n",
    "\n",
    "# --- Pré-requisito: df_vendas e df_pedidos ---\n",
    "# Assumindo que 'df_vendas' foi criado a partir do join e 'df_pedidos' está disponível\n",
    "# com as colunas renomeadas e tipos corrigidos.\n",
    "# Para evitar ambiguidade com a coluna 'order_id' no join, é melhor selecionar as colunas de df_pedidos\n",
    "# que são únicas para a análise de pedidos.\n",
    "df_pedidos_distinct = df_vendas.select(\n",
    "    \"order_id\", \"created_at\", \"valor_nf\", \"frete_cliente\", \"num_itens\"\n",
    ").distinct()\n",
    "\n",
    "\n",
    "# --- Análise 1: Vendas ao Longo do Tempo ---\n",
    "print(\"--- Análise 1: Faturamento e número de pedidos por dia ---\")\n",
    "vendas_diarias = df_pedidos_distinct.groupBy(date_trunc(\"day\", col(\"created_at\")).alias(\"dia\")) \\\n",
    "                                   .agg(\n",
    "                                       sum(\"valor_nf\").alias(\"faturamento_total\"),\n",
    "                                       count(\"order_id\").alias(\"numero_pedidos\")\n",
    "                                   ).orderBy(\"dia\")\n",
    "vendas_diarias.show()\n",
    "\n",
    "\n",
    "# --- Análise 2: Impacto da Promoção de Frete Grátis ---\n",
    "print(\"\\n--- Análise 2: Comparativo de pedidos com Frete Grátis vs. Pago ---\")\n",
    "analise_frete = df_pedidos_distinct.withColumn(\"tipo_frete\", when(col(\"frete_cliente\") > 0, \"Pago\").otherwise(\"Gratis\")) \\\n",
    "                                  .groupBy(\"tipo_frete\") \\\n",
    "                                  .agg(\n",
    "                                      avg(\"valor_nf\").alias(\"ticket_medio\"),\n",
    "                                      avg(\"num_itens\").alias(\"media_itens_por_pedido\"),\n",
    "                                      count(\"order_id\").alias(\"total_pedidos\")\n",
    "                                  )\n",
    "analise_frete.show()\n",
    "\n",
    "\n",
    "# --- Análise 3: Análise de Produtos em \"Queima de Estoque\" (stock_burning) ---\n",
    "print(\"\\n--- Análise 3: Produtos mais vendidos em 'Queima de Estoque' ---\")\n",
    "produtos_queima_estoque = df_vendas.filter(col(\"stock_burning\") == True) \\\n",
    "                                   .groupBy(\"material_name\", \"material_category\") \\\n",
    "                                   .agg(count(\"*\").alias(\"quantidade_vendida\")) \\\n",
    "                                   .orderBy(col(\"quantidade_vendida\").desc())\n",
    "produtos_queima_estoque.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a4a1fbf-75cd-4f7b-a4b1-1b451f30c7a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Análise do Desafio 1: Oscilações nas Vendas e Impacto de Promoções\n",
    "Objetivo: Entender o comportamento das vendas ao longo do tempo e medir se promoções como frete grátis ou queima de estoque (stock_burning) realmente funcionam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be32864c-7596-4004-90d1-b774bae158f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1.1. Análise de Vendas ao Longo do Tempo"
    }
   },
   "outputs": [],
   "source": [
    "#Vamos visualizar o faturamento e o número de pedidos por dia para identificar picos, quedas e padrões.\n",
    "#O que analisar nos resultados:\n",
    "# Picos de Vendas: Existem dias específicos (finais de semana, início/fim de mês) com vendas muito mais altas?\n",
    "# Sazonalidade: Você consegue observar algum padrão semanal ou mensal?\n",
    "# Visualização: Use a função de plotagem do Databricks para transformar essa tabela em um gráfico de linhas. Isso tornará as oscilações muito mais evidentes.\n",
    "# from pyspark.sql.functions import sum, count, col, date_trunc, avg, when\n",
    "\n",
    "# Usamos o df_pedidos para garantir que cada pedido seja contado apenas uma vez.\n",
    "vendas_diarias = df_pedidos.groupBy(\"created_at_data\") \\\n",
    "                           .agg(\n",
    "                               sum(\"valor_nf\").alias(\"faturamento_total\"),\n",
    "                               count(\"order_id\").alias(\"numero_pedidos\")\n",
    "                           ).orderBy(\"created_at_data\")\n",
    "\n",
    "print(\"--- Vendas Diárias (Faturamento e N° de Pedidos) ---\")\n",
    "display(vendas_diarias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4cba0a-277a-426b-8308-326cff3de1bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1.2. Impacto da Promoção de Frete Grátis"
    }
   },
   "outputs": [],
   "source": [
    "# Queremos saber se oferecer frete grátis incentiva os clientes a gastarem mais ou a comprarem mais itens.\n",
    "# O que analisar nos resultados:\n",
    "\n",
    "# Ticket Médio: O valor médio do pedido (ticket_medio) é significativamente maior nos pedidos com \"Frete Grátis\"? Se sim, a promoção está se pagando.\n",
    "\n",
    "# Média de Itens: Os clientes adicionam mais produtos ao carrinho para atingir o frete grátis? Compare a media_itens_por_pedido.\n",
    "# Segmentamos os pedidos em dois grupos: com frete pago e com frete grátis.\n",
    "analise_frete = df_pedidos.withColumn(\"tipo_frete\", when(col(\"frete_cliente\") > 0, \"Pago\").otherwise(\"Grátis\")) \\\n",
    "                          .groupBy(\"tipo_frete\") \\\n",
    "                          .agg(\n",
    "                              avg(\"valor_nf\").alias(\"ticket_medio\"),\n",
    "                              avg(\"num_itens\").alias(\"media_itens_por_pedido\"),\n",
    "                              count(\"order_id\").alias(\"total_pedidos\")\n",
    "                          )\n",
    "\n",
    "print(\"--- Análise de Impacto do Frete Grátis ---\")\n",
    "display(analise_frete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e89444-679e-47fd-b555-732618236596",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1.3. Análise de Itens em \"Queima de Estoque"
    }
   },
   "outputs": [],
   "source": [
    "# Vamos identificar quais produtos são mais vendidos quando marcados como stock_burning.\n",
    "# O que analisar nos resultados:\n",
    "\n",
    "# Top Produtos: Quais são os itens que mais se beneficiam dessa promoção? Isso pode ajudar a decidir quais produtos colocar em queima de estoque no futuro.\n",
    "# Usamos o df_vendas, pois a informação de stock_burning é a nível de item.\n",
    "produtos_queima_estoque = df_vendas.filter(col(\"stock_burning\") == True) \\\n",
    "                                   .groupBy(\"material_name\", \"material_category\") \\\n",
    "                                   .agg(count(\"*\").alias(\"unidades_vendidas\")) \\\n",
    "                                   .orderBy(col(\"unidades_vendidas\").desc())\n",
    "\n",
    "print(\"--- Produtos Mais Vendidos em Queima de Estoque ---\")\n",
    "display(produtos_queima_estoque.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61fd581-8427-4937-9462-5ea1f87f9bb0",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760195108168}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "1.4 Quais categorias ou produtos possuem maior impacto no faturamento?"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, desc, round\n",
    "# --- Ranking de Faturamento por Categoria ---\n",
    "faturamento_por_categoria = df_vendas.groupBy(\"material_category\") \\\n",
    "                                     .agg(round(sum(\"price\"), 2).alias(\"faturamento_total\"),\n",
    "                                         count(\"order_id\").alias(\"unidades_vendidas\")\n",
    "                                     ).orderBy(desc(\"faturamento_total\"))\n",
    "\n",
    "print(\"--- Ranking de Faturamento por Categoria de Produto ---\")\n",
    "display(faturamento_por_categoria)\n",
    "\n",
    "# --- Ranking de Faturamento por Produto ---\n",
    "\n",
    "faturamento_por_produto = df_vendas.groupBy(\"material_name\") \\\n",
    "                                   .agg(round(sum(\"price\"), 2).alias(\"faturamento_total\"),\n",
    "                                       count(\"order_id\").alias(\"unidades_vendidas\")\n",
    "                                   ).orderBy(desc(\"faturamento_total\"))\n",
    "\n",
    "print(\"\\n--- Top 20 Produtos por Faturamento ---\")\n",
    "display(faturamento_por_produto.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b58240-6121-4380-923d-ca1f92e2a04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Análise do Desafio 2: Rupturas de Estoque\n",
    "Objetivo: Identificar quais produtos estão em falta e, mais importante, qual o impacto potencial disso na receita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c9220a-7a0e-41c2-8d11-3ae3aeac1cae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2.1. Identificar Produtos Críticos em Ruptura"
    }
   },
   "outputs": [],
   "source": [
    "#Primeiro, listamos os produtos que deveriam estar à venda, mas estão com estoque zerado.\n",
    "produtos_ruptura = df_supply.filter(\n",
    "    (col(\"quantity\") == 0) &\n",
    "    (col(\"should_sell\") == True) &\n",
    "    (col(\"discontinued\") == False)\n",
    ").select(\"material_id\", \"material_name\", \"leadtime\")\n",
    "\n",
    "print(f\"Total de produtos em ruptura: {produtos_ruptura.count()}\")\n",
    "print(\"--- Amostra de Produtos em Ruptura de Estoque ---\")\n",
    "display(produtos_ruptura.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ab454f-d112-4fb6-bde3-ea9674a2a9dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "2.2. Priorizar a Reposição: Cruzando Ruptura com Demanda"
    }
   },
   "outputs": [],
   "source": [
    "# Um produto em ruptura só é um problema se ele vende bem. Vamos cruzar a lista de ruptura com o histórico de vendas para encontrar os casos mais críticos.\n",
    "# O que analisar nos resultados:\n",
    "\n",
    "# Lista de Prioridades: O topo desta tabela (ruptura_critica) são os produtos que você precisa repor com mais urgência. Eles estão em falta e têm uma alta demanda histórica.\n",
    "\n",
    "# leadtime vs. vendas_historicas: Um produto com vendas altíssimas e um leadtime (tempo de reposição) longo é um grande risco para a receita.\n",
    "# Contar as vendas históricas para cada material_id\n",
    "demanda_historica = df_vendas.groupBy(\"material_id\") \\\n",
    "                             .agg(count(\"order_id\").alias(\"vendas_historicas\"))\n",
    "\n",
    "# Juntar a lista de ruptura com a demanda para priorizar\n",
    "ruptura_critica = produtos_ruptura.join(demanda_historica, \"material_id\", \"inner\") \\\n",
    "                                  .orderBy(col(\"vendas_historicas\").desc())\n",
    "\n",
    "print(\"--- Produtos Mais Críticos para Reposição (Ruptura vs. Demanda) ---\")\n",
    "display(ruptura_critica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123b1052-3db8-422b-91bd-553432ee38f6",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760197386045}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "\"Existe correlação entre problemas de supply e cancelamento de pedidos?\""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# 1. Contar todos os pedidos por 'status_pedido'\n",
    "# O nome da variável foi alterado para 'status_count' para maior clareza\n",
    "status_count = df_pedidos.groupBy(\"status_pedido\") \\\n",
    "                         .agg(count(\"order_id\").alias(\"total_pedidos\"))\n",
    "\n",
    "print(\"--- Contagem de Pedidos por Status ---\")\n",
    "display(status_count)\n",
    "\n",
    "\n",
    "# 2. Isolar os itens de pedidos que foram efetivamente cancelados.\n",
    "# (Certifique-se de usar o nome exato que aparecer na tabela acima, ex: \"canceled\")\n",
    "status_cancelado = \"canceled\" # <-- AJUSTE AQUI se o nome for diferente\n",
    "pedidos_cancelados = df_vendas.filter(col(\"status_pedido\") == status_cancelado)\n",
    "\n",
    "\n",
    "# 3. Cruzar os itens cancelados com a lista de produtos em ruptura de estoque.\n",
    "itens_cancelados_vs_ruptura = pedidos_cancelados.join(\n",
    "    produtos_ruptura, # DataFrame da Análise 2.1\n",
    "    \"material_id\",\n",
    "    \"inner\"\n",
    ").groupBy(pedidos_cancelados[\"material_name\"]) \\\n",
    " .agg(count(\"*\").alias(\"cancelamentos_de_itens_em_ruptura\")) \\\n",
    " .orderBy(col(\"cancelamentos_de_itens_em_ruptura\").desc())\n",
    "\n",
    "print(f\"\\n--- Itens em Ruptura que foram Cancelados (Status = '{status_cancelado}') ---\")\n",
    "display(itens_cancelados_vs_ruptura)\n",
    "\n",
    "total_pedidos = df_pedidos.count()\n",
    "pedidos_cancelados_count = df_pedidos.filter(col(\"status_pedido\") == status_cancelado).count()\n",
    "taxa_cancelamento_percentual = (pedidos_cancelados_count / total_pedidos) * 100\n",
    "\n",
    "print(taxa_cancelamento_percentual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02c3d915-2184-4f24-af79-4aae1d248913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- Passo 1: Defina o nome exato do status de cancelamento ---\n",
    "# (Verifique na tabela da análise anterior se é \"canceled\", \"Cancelado\", etc.)\n",
    "status_cancelado = \"canceled\" \n",
    "\n",
    "# --- Passo 2: Calcule o número total de pedidos ---\n",
    "total_pedidos = df_pedidos.count()\n",
    "\n",
    "# --- Passo 3: Calcule o número de pedidos com o status de cancelamento ---\n",
    "pedidos_cancelados_count = df_pedidos.filter(col(\"status_pedido\") == status_cancelado).count()\n",
    "\n",
    "# --- Passo 4: Calcule e exiba a taxa de cancelamento ---\n",
    "print(\"--- Cálculo da Taxa de Cancelamento ---\")\n",
    "\n",
    "# Verificação para evitar divisão por zero, caso o DataFrame esteja vazio\n",
    "if total_pedidos > 0:\n",
    "    taxa_cancelamento_percentual = (pedidos_cancelados_count / total_pedidos) * 100\n",
    "    \n",
    "    print(f\"Total de pedidos no DataFrame: {total_pedidos}\")\n",
    "    print(f\"Total de pedidos cancelados ('{status_cancelado}'): {pedidos_cancelados_count}\")\n",
    "    print(f\"Taxa de cancelamento: {taxa_cancelamento_percentual:.2f}%\")\n",
    "else:\n",
    "    print(\"Não há pedidos para calcular a taxa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bbe5fc-9c64-41b9-92d6-0942c6ecaaa3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760198492480}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, desc, lit\n",
    "\n",
    "# --- Passo 1: Calcular o total de cancelamentos para CADA item ---\n",
    "# Filtramos o df_vendas para pegar apenas os itens de pedidos cancelados\n",
    "# e depois contamos quantas vezes cada 'material_name' aparece.\n",
    "\n",
    "status_cancelado = \"canceled\" # Confirme se este é o nome correto do status\n",
    "\n",
    "cancelamentos_por_item = df_vendas.filter(col(\"status_pedido\") == status_cancelado) \\\n",
    "                                  .groupBy(\"material_name\") \\\n",
    "                                  .agg(count(\"*\").alias(\"total_cancelado\"))\n",
    "\n",
    "# --- Passo 2: Preparar a lista de produtos em ruptura para o join ---\n",
    "# Vamos apenas selecionar a coluna que precisamos e adicionar uma \"flag\"\n",
    "# para indicar que esses itens estão em ruptura.\n",
    "\n",
    "info_ruptura = produtos_ruptura.select(\n",
    "    \"material_name\",\n",
    "    lit(True).alias(\"em_ruptura\") # lit() cria uma coluna com um valor literal (constante)\n",
    ")\n",
    "\n",
    "# --- Passo 3: Criar o DataFrame final combinando as informações ---\n",
    "# Usamos um LEFT JOIN para garantir que todos os itens cancelados apareçam na lista,\n",
    "# mesmo que eles NÃO estejam em ruptura de estoque.\n",
    "\n",
    "df_analise_cancelamento = cancelamentos_por_item.join(\n",
    "    info_ruptura,\n",
    "    \"material_name\", # A chave da junção\n",
    "    \"left\"\n",
    ").na.fill(False, [\"em_ruptura\"]) \\\n",
    " .orderBy(desc(\"total_cancelado\")) # Ordena para ver os mais problemáticos primeiro\n",
    "\n",
    "# --- Passo 4: Exibir o resultado ---\n",
    "print(\"--- Análise de Cancelamentos por Item vs. Ruptura de Estoque ---\")\n",
    "display(df_analise_cancelamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a32b94ff-e101-42a6-adfe-0109444ccc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Análise do Desafio 3: Problemas Logísticos\n",
    "Objetivo: Medir a eficiência da entrega, identificar atrasos e comparar o desempenho de transportadoras e regiões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1f13c5-50ab-4ab5-8af3-957d52e9cc28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3.1. Calcular Tempos do Ciclo de Entrega"
    }
   },
   "outputs": [],
   "source": [
    "# Vamos medir o tempo que um pedido leva para sair do armazém e o tempo que a transportadora leva para entregá-lo.\n",
    "\n",
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "# Calculamos a diferença em dias entre as etapas do pedido\n",
    "df_pedidos_com_prazos = df_pedidos.withColumn(\"tempo_despacho_dias\", datediff(col(\"data_envio\"), col(\"created_at_data\"))) \\\n",
    "                                 .withColumn(\"tempo_transporte_dias\", datediff(col(\"data_entrega\"), col(\"data_envio\")))\n",
    "\n",
    "# Calcular médias gerais\n",
    "medias_logisticas = df_pedidos_com_prazos.agg(\n",
    "    avg(\"tempo_despacho_dias\").alias(\"media_dias_para_despacho\"),\n",
    "    avg(\"tempo_transporte_dias\").alias(\"media_dias_em_transporte\")\n",
    ")\n",
    "\n",
    "print(\"--- Médias de Tempo do Ciclo Logístico ---\")\n",
    "display(medias_logisticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34c52e6-d297-4981-8aee-218f71757128",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3.2. Análise de Atrasos e Desempenho por Transportadora"
    }
   },
   "outputs": [],
   "source": [
    "# Estamos cumprindo os prazos? Qual transportadora é mais rápida e qual tem mais atrasos?\n",
    "# O que analisar nos resultados:\n",
    "\n",
    "# Ranking de Transportadoras: Qual transportadora tem o menor tempo_medio_entrega e o menor percentual_atrasos? Isso pode ser crucial para renegociar contratos.\n",
    "\n",
    "# Análise por Estado: Você pode facilmente adaptar a consulta acima para fazer um groupBy(\"Estado\") em vez de groupBy(\"Transportadora\") e descobrir quais estados têm os maiores prazos ou taxas de atraso.\n",
    "# Criar uma flag para pedidos atrasados\n",
    "df_pedidos_com_atraso = df_pedidos_com_prazos.withColumn(\n",
    "    \"atrasado\",\n",
    "    when(col(\"data_entrega\") > col(\"prazo_entrega_transportadora\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Agrupar por transportadora para analisar o desempenho\n",
    "desempenho_transportadora = df_pedidos_com_atraso.groupBy(\"Transportadora\") \\\n",
    "    .agg(\n",
    "        avg(\"tempo_transporte_dias\").alias(\"tempo_medio_entrega\"),\n",
    "        (sum(\"atrasado\") / count(\"order_id\") * 100).alias(\"percentual_atrasos\"),\n",
    "        count(\"order_id\").alias(\"total_entregas\")\n",
    "    ).orderBy(col(\"tempo_medio_entrega\"))\n",
    "\n",
    "print(\"--- Desempenho por Transportadora ---\")\n",
    "display(desempenho_transportadora)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
